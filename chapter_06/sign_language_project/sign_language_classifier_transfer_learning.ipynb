{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:deep_learning] *",
      "language": "python",
      "name": "conda-env-deep_learning-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "sign_language_classifier_transfer_learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTQecEDqwzbv"
      },
      "source": [
        "# [Deep Learning for Vision Systems](https://www.manning.com/books/deep-learning-for-vision-systems?a_aid=compvisionbookcom&a_bid=90abff15) Book\n",
        "\n",
        "\n",
        "## Chapter 6 Project: Sign language exercise\n",
        "\n",
        "---\n",
        "### 1. Import the libraries that we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa5C4A3Wwzbx",
        "outputId": "40bd5b2f-8801-4163-cc3d-2c45fc149d1c"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.applications import vgg16\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.metrics import categorical_crossentropy\n",
        "\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJu1cGrlwzbz"
      },
      "source": [
        "### 2. Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPgr621Ewzbz"
      },
      "source": [
        "train_path  = 'dataset/train'\n",
        "valid_path  = 'dataset/valid'\n",
        "test_path  = 'dataset/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay6UgUVmwzb0",
        "outputId": "daf08f84-7bcc-44da-c546-3c4ec7f30e47"
      },
      "source": [
        "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
        "# The data will be looped over (in batches).\n",
        "# in this example, we won't be doing any image augmentation\n",
        "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
        "                                                         target_size=(224,224), \n",
        "                                                         batch_size=10)\n",
        "\n",
        "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
        "                                                         target_size=(224,224), \n",
        "                                                         batch_size=30)\n",
        "\n",
        "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
        "                                                        target_size=(224,224), \n",
        "                                                        batch_size=50, \n",
        "                                                        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1712 images belonging to 10 classes.\n",
            "Found 300 images belonging to 10 classes.\n",
            "Found 50 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXSIkFsjwzb0"
      },
      "source": [
        "### 3. VGG16 base model pre-trained on ImageNet dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i3avmzxwzb1",
        "outputId": "d624872d-93b0-41ea-91ed-aae2a44909ad"
      },
      "source": [
        "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kdDmO98wzb1"
      },
      "source": [
        "### 2. freeze the classification layers in the base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-NIDjo0wzb2",
        "outputId": "8650e6f3-5549-438f-8912-68a7d544440e"
      },
      "source": [
        "# iterate through its layers and lock them to make them not trainable with this code\n",
        "for layer in base_model.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heeKrNS4wzb2",
        "outputId": "085190f2-2e5a-4861-8d1e-3d5b576b0892"
      },
      "source": [
        "# use “get_layer” method to save the last layer of the network\n",
        "last_layer = base_model.get_layer('global_average_pooling2d_1')\n",
        "\n",
        "# save the output of the last layer to be the input of the next layer\n",
        "last_output = last_layer.output\n",
        "\n",
        "# add our new softmax layer with 3 hidden units\n",
        "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
        "\n",
        "# instantiate a new_model using keras’s Model class\n",
        "new_model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# print the new_model summary\n",
        "new_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 14,719,818\n",
            "Trainable params: 7,084,554\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29_h26vBwzb3"
      },
      "source": [
        "### 4. Train the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8A9p31Pwzb3"
      },
      "source": [
        "new_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GNDzpvkmwzb3",
        "outputId": "b2a399b0-9653-47eb-820e-3a3ff60dab19"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
        "\n",
        "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
        "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "18/18 [==============================] - 40s 2s/step - loss: 3.2263 - acc: 0.1833 - val_loss: 2.0674 - val_acc: 0.1667\n",
            "Epoch 2/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 2.0311 - acc: 0.1833 - val_loss: 1.7330 - val_acc: 0.3000\n",
            "Epoch 3/150\n",
            "18/18 [==============================] - 42s 2s/step - loss: 1.5741 - acc: 0.4500 - val_loss: 1.5577 - val_acc: 0.4000\n",
            "Epoch 4/150\n",
            "18/18 [==============================] - 42s 2s/step - loss: 1.3068 - acc: 0.5111 - val_loss: 0.9856 - val_acc: 0.7333\n",
            "Epoch 5/150\n",
            "18/18 [==============================] - 43s 2s/step - loss: 1.1563 - acc: 0.6389 - val_loss: 0.7637 - val_acc: 0.7333\n",
            "Epoch 6/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.8414 - acc: 0.6722 - val_loss: 0.7550 - val_acc: 0.8000\n",
            "Epoch 7/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.5982 - acc: 0.8444 - val_loss: 0.7910 - val_acc: 0.6667\n",
            "Epoch 8/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.3804 - acc: 0.8722 - val_loss: 0.7376 - val_acc: 0.8667\n",
            "Epoch 9/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.5048 - acc: 0.8222 - val_loss: 0.2677 - val_acc: 0.9000\n",
            "Epoch 10/150\n",
            "18/18 [==============================] - 39s 2s/step - loss: 0.2383 - acc: 0.9276 - val_loss: 0.2844 - val_acc: 0.9000\n",
            "Epoch 11/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.1163 - acc: 0.9778 - val_loss: 0.0775 - val_acc: 1.0000\n",
            "Epoch 12/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.1377 - acc: 0.9667 - val_loss: 0.5140 - val_acc: 0.9333\n",
            "Epoch 13/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.0955 - acc: 0.9556 - val_loss: 0.1783 - val_acc: 0.9333\n",
            "Epoch 14/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.1785 - acc: 0.9611 - val_loss: 0.0704 - val_acc: 0.9333\n",
            "Epoch 15/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.0533 - acc: 0.9778 - val_loss: 0.4692 - val_acc: 0.8667\n",
            "Epoch 16/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.0809 - acc: 0.9778 - val_loss: 0.0447 - val_acc: 1.0000\n",
            "Epoch 17/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.0834 - acc: 0.9722 - val_loss: 0.0284 - val_acc: 1.0000\n",
            "Epoch 18/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.1022 - acc: 0.9611 - val_loss: 0.0177 - val_acc: 1.0000\n",
            "Epoch 19/150\n",
            "18/18 [==============================] - 41s 2s/step - loss: 0.1134 - acc: 0.9667 - val_loss: 0.0595 - val_acc: 1.0000\n",
            "Epoch 20/150\n",
            "18/18 [==============================] - 39s 2s/step - loss: 0.0676 - acc: 0.9777 - val_loss: 0.0862 - val_acc: 0.9667\n",
            "Epoch 21/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-403113af30b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = new_model2.fit_generator(train_batches, steps_per_epoch=18,\n\u001b[0;32m----> 6\u001b[0;31m                    validation_data=valid_batches, validation_steps=3, epochs=150, verbose=1, callbacks=[checkpointer])\n\u001b[0m",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPWxdDdmwzb4"
      },
      "source": [
        "### 5. create the confusion matrix to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hbomPAUwzb4"
      },
      "source": [
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    paths = np.array(data['filenames'])\n",
        "    targets = np_utils.to_categorical(np.array(data['target']))\n",
        "    return paths, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x6bd8evwzb4"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "\n",
        "test_files, test_targets = load_dataset('dataset/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xkbx56iwzb5",
        "outputId": "e9b653f2-781c-4db4-dbbe-cbc4d72ea5b9"
      },
      "source": [
        "from keras.preprocessing import image  \n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tqdm import tqdm\n",
        "\n",
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)\n",
        "\n",
        "test_tensors = preprocess_input(paths_to_tensor(test_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 386.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Emx5rdfwzb5"
      },
      "source": [
        "new_model.load_weights('signlanguage.model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaHt_mELwzb5",
        "outputId": "e7febd10-6a62-49ee-e7ca-152ef63a6fce"
      },
      "source": [
        "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 8s 154ms/step\n",
            "\n",
            "Testing loss: 0.0574\n",
            "Testing accuracy: 0.9800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vZVuDNIwzb5",
        "outputId": "9920a7af-cf6d-4b9e-cf83-875938395dfe"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "cm = confusion_matrix(np.argmax(test_targets, axis=1),\n",
        "                      np.argmax(new_model.predict(test_tensors), axis=1))\n",
        "plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "indexes = np.arange(len(cm_labels))\n",
        "for i in indexes:\n",
        "    for j in indexes:\n",
        "        plt.text(j, i, cm[i, j])\n",
        "plt.xticks(indexes, cm_labels, rotation=90)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.yticks(indexes, cm_labels)\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLg26ybGwzb6"
      },
      "source": [
        "### The End!"
      ]
    }
  ]
}